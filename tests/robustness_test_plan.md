# 异常与鲁棒性测试计划

本测试计划旨在验证系统在各种异常情况下的稳定性和恢复能力。

## 1. 测试范围

*   **网络异常**: 模拟网络断开、超时、DNS 解析失败。
*   **第三方服务异常**: 模拟 Google Scholar, Arxiv, Scopus 等数据源的限流 (429)、服务不可用 (500/503)、响应格式变更。
*   **LLM 服务异常**: 模拟 OpenAI API 超时、额度耗尽、模型过载。
*   **并发与负载**: 模拟多用户并发请求、大量爬虫任务并行。
*   **数据一致性**: 模拟爬虫任务中途失败后的数据状态。

## 2. 测试场景与用例

### 2.1 网络异常测试

| ID | 场景 | 预期结果 | 测试方法 |
|---|---|---|---|
| NET-01 | 爬虫请求超时 | 爬虫应捕获超时异常，记录日志，并根据策略重试或标记任务失败（非崩溃）。 | 使用 `mock` 模拟 `requests.get` 超时。 |
| NET-02 | 网络完全断开 | 系统应提示网络错误，前端不应白屏，后端不应 Crash。 | 断开本地网络或在 Docker 容器中禁用网络。 |

### 2.2 第三方数据源异常

| ID | 场景 | 预期结果 | 测试方法 |
|---|---|---|---|
| SRC-01 | Google Scholar 429 (Too Many Requests) | 爬虫应识别 429，触发指数退避重试 (Exponential Backoff) 或暂停任务。 | Mock 响应状态码 429。 |
| SRC-02 | Arxiv API 返回畸形 XML | 解析器应捕获 XML 解析错误，跳过该条目或标记失败，不影响其他条目。 | Mock 返回损坏的 XML 数据。 |
| SRC-03 | Scopus API Key 无效 | 爬虫应记录认证失败日志，停止该源的抓取，不影响其他源。 | 配置无效的 API Key 进行测试。 |

### 2.3 LLM 服务异常

| ID | 场景 | 预期结果 | 测试方法 |
|---|---|---|---|
| LLM-01 | OpenAI API 超时 | 综述生成服务应捕获超时，提示用户稍后重试。 | Mock `openai.ChatCompletion.create` 超时。 |
| LLM-02 | Context Window 超限 | 系统应在发送前估算 Token 数，或捕获 API 报错并尝试截断/分块处理。 | 输入超长文本进行综述生成。 |
| LLM-03 | API Key 额度耗尽 | 系统应明确提示额度不足，而非通用的“内部错误”。 | 使用无额度的 Key 测试。 |

### 2.4 任务调度与恢复

| ID | 场景 | 预期结果 | 测试方法 |
|---|---|---|---|
| JOB-01 | 爬虫任务执行中服务重启 | 服务重启后，未完成的任务应能被检测到并标记为“失败”或“中断”，支持手动恢复。 | 启动长任务，强制 kill 后端进程，重启后检查状态。 |
| JOB-02 | 数据库连接断开 | 后端应尝试重连，若失败则返回 500，不应导致数据损坏。 | 停止数据库容器或重命名数据库文件。 |

## 3. 自动化测试脚本 (Planned)

我们将编写 Python 脚本来模拟上述部分场景，特别是针对 Crawler 和 LLM Service 的单元测试/集成测试。

*   `tests/test_crawler_robustness.py`: 针对爬虫的异常模拟。
*   `tests/test_llm_robustness.py`: 针对 LLM 的异常模拟。